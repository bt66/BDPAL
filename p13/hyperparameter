{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyNxAcuHzpACfstjbdraG1Io"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":[],"metadata":{"id":"mS6zJM6Xhl4N"}},{"cell_type":"code","source":["!pip install -q findspark\n","!pip install pyspark"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"D83848Jxh8mU","outputId":"3f0875a8-9206-40e9-8dc8-de932a190d8c"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting pyspark\n","  Downloading pyspark-3.3.1.tar.gz (281.4 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m281.4/281.4 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Collecting py4j==0.10.9.5\n","  Downloading py4j-0.10.9.5-py2.py3-none-any.whl (199 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.7/199.7 KB\u001b[0m \u001b[31m16.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hBuilding wheels for collected packages: pyspark\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":35},"id":"eBeWgMuYhi-G","executionInfo":{"status":"ok","timestamp":1672814932434,"user_tz":-420,"elapsed":5,"user":{"displayName":"3714_Muhammad Bastian Hanafi","userId":"06608780676813486786"}},"outputId":"50652433-7d68-416d-e91a-47d0eb279f3b"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'3.3.1'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":5}],"source":["# creating Spark session\n","from pyspark.sql import SparkSession\n","\n","spark = SparkSession.builder.getOrCreate()\n","#spark = SparkSession.builder\\\n","#        .master(\"local\")\\\n","#        .appName(\"RDD\")\\\n","#        .config('spark.ui.port', '4050')\\\n","#        .getOrCreate()\n","\n","spark\n","sc = spark.sparkContext\n","\n","sc.version"]},{"cell_type":"code","source":["from pyspark.ml import Pipeline\n","from pyspark.ml.classification import LogisticRegression\n","from pyspark.ml.evaluation import BinaryClassificationEvaluator\n","#  mengubah kata ke angka\n","from pyspark.ml.feature import HashingTF, Tokenizer\n","from pyspark.ml.tuning import CrossValidator, ParamGridBuilder\n","\n","# Prepare training documents, which are labeled.\n","training = spark.createDataFrame([\n","    (0, \"a b c d e spark\", 1.0),\n","    (1, \"b d\", 0.0),\n","    (2, \"spark f g h\", 1.0),\n","    (3, \"hadoop mapreduce\", 0.0),\n","    (4, \"b spark who\", 1.0),\n","    (5, \"g d a y\", 0.0),\n","    (6, \"spark fly\", 1.0),\n","    (7, \"was mapreduce\", 0.0),\n","    (8, \"e spark program\", 1.0),\n","    (9, \"a e c l\", 0.0),\n","    (10, \"spark compile\", 1.0),\n","    (11, \"hadoop software\", 0.0)\n","], [\"id\", \"text\", \"label\"])"],"metadata":{"id":"6fDZTHH6iNf7"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["training.show()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"yOyX2tHVjW8j","executionInfo":{"status":"ok","timestamp":1672815153297,"user_tz":-420,"elapsed":3652,"user":{"displayName":"3714_Muhammad Bastian Hanafi","userId":"06608780676813486786"}},"outputId":"5477d27a-46a3-4d2c-823e-d92fa8c3e0ca"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["+---+----------------+-----+\n","| id|            text|label|\n","+---+----------------+-----+\n","|  0| a b c d e spark|  1.0|\n","|  1|             b d|  0.0|\n","|  2|     spark f g h|  1.0|\n","|  3|hadoop mapreduce|  0.0|\n","|  4|     b spark who|  1.0|\n","|  5|         g d a y|  0.0|\n","|  6|       spark fly|  1.0|\n","|  7|   was mapreduce|  0.0|\n","|  8| e spark program|  1.0|\n","|  9|         a e c l|  0.0|\n","| 10|   spark compile|  1.0|\n","| 11| hadoop software|  0.0|\n","+---+----------------+-----+\n","\n"]}]},{"cell_type":"code","source":["# Configure an ML pipeline, which consists of tree stages: tokenizer, hashingTF, and lr.\n","tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\n","hashingTF = HashingTF(inputCol=tokenizer.getOutputCol(), outputCol=\"features\")\n","lr = LogisticRegression(maxIter=10)\n","pipeline = Pipeline(stages=[tokenizer, hashingTF, lr])\n","\n","# We now treat the Pipeline as an Estimator, wrapping it in a CrossValidator instance.\n","# This will allow us to jointly choose parameters for all Pipeline stages.\n","# A CrossValidator requires an Estimator, a set of Estimator ParamMaps, and an Evaluator.\n","# We use a ParamGridBuilder to construct a grid of parameters to search over.\n","# With 3 values for hashingTF.numFeatures and 2 values for lr.regParam,\n","# this grid will have 3 x 2 = 6 parameter settings for CrossValidator to choose from.\n","paramGrid = ParamGridBuilder() \\\n","    .addGrid(hashingTF.numFeatures, [10, 100, 1000]) \\\n","    .addGrid(lr.regParam, [0.1, 0.01]) \\\n","    .build()\n","\n","crossval = CrossValidator(estimator=pipeline,\n","                          estimatorParamMaps=paramGrid,\n","                          evaluator=BinaryClassificationEvaluator(),\n","                          numFolds=2)  # use 3+ folds in practice"],"metadata":{"id":"sSYH3RgBjX6H"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","# melihat hasil tokenize\n","tokenizer.transform(training).head(10)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"isZv0mTrkWkh","executionInfo":{"status":"ok","timestamp":1672815419855,"user_tz":-420,"elapsed":2108,"user":{"displayName":"3714_Muhammad Bastian Hanafi","userId":"06608780676813486786"}},"outputId":"6a8b24f0-e31a-4a0b-f26d-b6396b137fbc"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["[Row(id=0, text='a b c d e spark', label=1.0, words=['a', 'b', 'c', 'd', 'e', 'spark']),\n"," Row(id=1, text='b d', label=0.0, words=['b', 'd']),\n"," Row(id=2, text='spark f g h', label=1.0, words=['spark', 'f', 'g', 'h']),\n"," Row(id=3, text='hadoop mapreduce', label=0.0, words=['hadoop', 'mapreduce']),\n"," Row(id=4, text='b spark who', label=1.0, words=['b', 'spark', 'who']),\n"," Row(id=5, text='g d a y', label=0.0, words=['g', 'd', 'a', 'y']),\n"," Row(id=6, text='spark fly', label=1.0, words=['spark', 'fly']),\n"," Row(id=7, text='was mapreduce', label=0.0, words=['was', 'mapreduce']),\n"," Row(id=8, text='e spark program', label=1.0, words=['e', 'spark', 'program']),\n"," Row(id=9, text='a e c l', label=0.0, words=['a', 'e', 'c', 'l'])]"]},"metadata":{},"execution_count":10}]},{"cell_type":"code","source":["tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"words\")\n","hashingTF = HashingTF(inputCol=tokenizer.getOutputCol(), outputCol=\"features\")\n","\n","# hashingTF.transform(tokenizer).head().features"],"metadata":{"id":"YNT28Y2UkcTQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["\n","hashingTF.indexOf(\"a\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"QfeNAjLGke2I","executionInfo":{"status":"ok","timestamp":1672815442671,"user_tz":-420,"elapsed":3,"user":{"displayName":"3714_Muhammad Bastian Hanafi","userId":"06608780676813486786"}},"outputId":"a037ef6a-75f9-469f-b1f9-84c4ea38fced"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["107107"]},"metadata":{},"execution_count":12}]},{"cell_type":"code","source":["\n","# Run cross-validation, and choose the best set of parameters.\n","cvModel = crossval.fit(training)"],"metadata":{"id":"cVENvZfKkg7A"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["bestModel = cvModel.bestModel\n","bestModel"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jfjVNHdXkjAQ","executionInfo":{"status":"ok","timestamp":1672816143106,"user_tz":-420,"elapsed":14,"user":{"displayName":"3714_Muhammad Bastian Hanafi","userId":"06608780676813486786"}},"outputId":"b38a5ea9-f5c5-4563-cb1a-91ea5ab79e88"},"execution_count":null,"outputs":[{"output_type":"execute_result","data":{"text/plain":["PipelineModel_b8b3f5c4e9d2"]},"metadata":{},"execution_count":17}]},{"cell_type":"code","source":["\n","# Prepare test documents, which are unlabeled.\n","test = spark.createDataFrame([\n","    (4, \"spark i j k\"),\n","    (5, \"l m n\"),\n","    (6, \"mapreduce spark\"),\n","    (7, \"apache hadoop\")\n","], [\"id\", \"text\"])\n","\n","# Make predictions on test documents. cvModel uses the best model found (lrModel).\n","prediction = cvModel.transform(test)\n","selected = prediction.select(\"id\", \"text\", \"probability\", \"prediction\")\n","for row in selected.collect():\n","    print(row)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LsiiTup3kkqQ","executionInfo":{"status":"ok","timestamp":1672815482802,"user_tz":-420,"elapsed":578,"user":{"displayName":"3714_Muhammad Bastian Hanafi","userId":"06608780676813486786"}},"outputId":"d8591db1-d56f-49e2-e7e9-9effb2808376"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Row(id=4, text='spark i j k', probability=DenseVector([0.0002, 0.9998]), prediction=1.0)\n","Row(id=5, text='l m n', probability=DenseVector([0.9066, 0.0934]), prediction=0.0)\n","Row(id=6, text='mapreduce spark', probability=DenseVector([0.699, 0.301]), prediction=0.0)\n","Row(id=7, text='apache hadoop', probability=DenseVector([0.8794, 0.1206]), prediction=0.0)\n"]}]}]}