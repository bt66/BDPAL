{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"1yylLaXzmG_JE419W2dPUkWFzXueV5sxz","authorship_tag":"ABX9TyM3aBUSHyyTZmTBbNlTWWmT"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"id":"I2u8Dc1kuvTb","executionInfo":{"status":"ok","timestamp":1669764888068,"user_tz":-420,"elapsed":667,"user":{"displayName":"3714_Muhammad Bastian Hanafi","userId":"06608780676813486786"}}},"outputs":[],"source":["import pandas as pd\n","import numpy as np"]},{"cell_type":"code","source":["!cp /content/drive/MyDrive/kuliah/Smester\\ \\5/big\\ \\data\\ \\lanjut/spark-3.0.0-bin-hadoop3.2.tgz ."],"metadata":{"id":"X6YcrcSHvQ9A","executionInfo":{"status":"ok","timestamp":1669764933961,"user_tz":-420,"elapsed":4646,"user":{"displayName":"3714_Muhammad Bastian Hanafi","userId":"06608780676813486786"}}},"execution_count":2,"outputs":[]},{"cell_type":"code","source":["!tar xf spark-3.0.0-bin-hadoop3.2.tgz\n","!java -version"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"RRmXamHovyqP","executionInfo":{"status":"ok","timestamp":1669764963262,"user_tz":-420,"elapsed":4104,"user":{"displayName":"3714_Muhammad Bastian Hanafi","userId":"06608780676813486786"}},"outputId":"fbedfe5d-1891-475d-db5a-dfc22ce8539d"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["openjdk version \"11.0.17\" 2022-10-18\n","OpenJDK Runtime Environment (build 11.0.17+8-post-Ubuntu-1ubuntu218.04)\n","OpenJDK 64-Bit Server VM (build 11.0.17+8-post-Ubuntu-1ubuntu218.04, mixed mode, sharing)\n"]}]},{"cell_type":"code","source":["!pip install -q findspark\n","!pip install pyspark"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"ZK3eirWSv252","executionInfo":{"status":"ok","timestamp":1669765028109,"user_tz":-420,"elapsed":57528,"user":{"displayName":"3714_Muhammad Bastian Hanafi","userId":"06608780676813486786"}},"outputId":"7b06b8ed-50b2-4436-c501-d5b6f43cd605"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting pyspark\n","  Downloading pyspark-3.3.1.tar.gz (281.4 MB)\n","\u001b[K     |████████████████████████████████| 281.4 MB 42 kB/s \n","\u001b[?25hCollecting py4j==0.10.9.5\n","  Downloading py4j-0.10.9.5-py2.py3-none-any.whl (199 kB)\n","\u001b[K     |████████████████████████████████| 199 kB 51.6 MB/s \n","\u001b[?25hBuilding wheels for collected packages: pyspark\n","  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for pyspark: filename=pyspark-3.3.1-py2.py3-none-any.whl size=281845512 sha256=2306ac90633155e1fd2033d24aa11b96997b193596679ff40ddfd4cb433c8f3a\n","  Stored in directory: /root/.cache/pip/wheels/42/59/f5/79a5bf931714dcd201b26025347785f087370a10a3329a899c\n","Successfully built pyspark\n","Installing collected packages: py4j, pyspark\n","Successfully installed py4j-0.10.9.5 pyspark-3.3.1\n"]}]},{"cell_type":"code","source":["import os\n","os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\n","os.environ[\"SPARK_HOME\"] = \"/content/spark-3.0.0-bin-hadoop3.2\""],"metadata":{"id":"X8sK6ZX8v_ds","executionInfo":{"status":"ok","timestamp":1669765032332,"user_tz":-420,"elapsed":685,"user":{"displayName":"3714_Muhammad Bastian Hanafi","userId":"06608780676813486786"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["from pyspark.sql import SparkSession\n","spark = SparkSession.builder.getOrCreate()\n","sc = spark.sparkContext\n","print(spark)\n","print(spark.version)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"tsRg6Iw1wITB","executionInfo":{"status":"ok","timestamp":1669765097909,"user_tz":-420,"elapsed":7412,"user":{"displayName":"3714_Muhammad Bastian Hanafi","userId":"06608780676813486786"}},"outputId":"70912071-b56b-4226-dc56-408afc618497"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["<pyspark.sql.session.SparkSession object at 0x7f287ee09ad0>\n","3.3.1\n"]}]},{"cell_type":"code","source":["# import library\n","from pyspark.ml.feature import HashingTF, IDF, Tokenizer\n","\n","sentenceData = spark.createDataFrame([\n","    (0.0, \"Hi I heard about Spark\"),\n","    (0.0, \"I wish Java could use case classes\"),\n","    (1.0, \"Logistic regression models are neat\")\n","], [\"label\", \"sentence\"])\n","\n","tokenizer = Tokenizer(inputCol=\"sentence\", outputCol=\"words\")\n","wordsData = tokenizer.transform(sentenceData)\n","\n","hashingTF = HashingTF(inputCol=\"words\", outputCol=\"rawFeatures\", numFeatures=20)\n","featurizedData = hashingTF.transform(wordsData)\n","# alternatively, CountVectorizer can also be used to get term frequency vectors\n","\n","idf = IDF(inputCol=\"rawFeatures\", outputCol=\"features\")\n","idfModel = idf.fit(featurizedData)\n","rescaledData = idfModel.transform(featurizedData)\n","\n","rescaledData.select(\"label\", \"features\").show()"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Qf349cZ8wjke","executionInfo":{"status":"ok","timestamp":1669766035258,"user_tz":-420,"elapsed":1811,"user":{"displayName":"3714_Muhammad Bastian Hanafi","userId":"06608780676813486786"}},"outputId":"0e2d9864-38de-4951-d57e-95f432d985a6"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["+-----+--------------------+\n","|label|            features|\n","+-----+--------------------+\n","|  0.0|(20,[6,8,13,16],[...|\n","|  0.0|(20,[0,2,7,13,15,...|\n","|  1.0|(20,[3,4,6,11,19]...|\n","+-----+--------------------+\n","\n"]}]},{"cell_type":"code","source":["from pyspark.ml.feature import Word2Vec\n","\n","# Input data: Each row is a bag of words from a sentence or document.\n","documentDF = spark.createDataFrame([\n","    (\"Hi I heard about Spark\".split(\" \"), ),\n","    (\"I wish Java could use case classes\".split(\" \"), ),\n","    (\"Logistic regression models are neat\".split(\" \"), )\n","], [\"text\"])\n"," \n","# Learn a mapping from words to Vectors.\n","word2Vec = Word2Vec(vectorSize=3, minCount=0, inputCol=\"text\", outputCol=\"result\")\n","model = word2Vec.fit(documentDF)\n","\n","result = model.transform(documentDF)\n","for row in result.collect():\n","    text, vector = row\n","    print(\"Text: [%s] => \\nVector: %s\\n\" % (\", \".join(text), str(vector)))"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Vtgahb1w1OxP","executionInfo":{"status":"ok","timestamp":1669767199930,"user_tz":-420,"elapsed":2496,"user":{"displayName":"3714_Muhammad Bastian Hanafi","userId":"06608780676813486786"}},"outputId":"ce7c7df6-e5bb-46d8-f1da-661ebdb02174"},"execution_count":9,"outputs":[{"output_type":"stream","name":"stdout","text":["Text: [Hi, I, heard, about, Spark] => \n","Vector: [-0.08430578038096428,-0.10903458818793298,0.011594421043992043]\n","\n","Text: [I, wish, Java, could, use, case, classes] => \n","Vector: [0.02311639008777482,-0.012267496436834335,-0.010578347902212824]\n","\n","Text: [Logistic, regression, models, are, neat] => \n","Vector: [0.041416072845458986,0.05713239423930645,-0.009863410668913275]\n","\n"]}]},{"cell_type":"code","source":["from pyspark.ml.feature import CountVectorizer\n","\n","# Input data: Each row is a bag of words with a ID.\n","df = spark.createDataFrame([\n","    (0, \"c c a b\".split(\" \")),\n","    (1, \"a b b c a\".split(\" \"))\n","], [\"id\", \"words\"])\n","\n","# fit a CountVectorizerModel from the corpus.\n","cv = CountVectorizer(inputCol=\"words\", outputCol=\"features\", vocabSize=3, minDF=2.0)\n","\n","model = cv.fit(df)\n","\n","result = model.transform(df)\n","result.show(truncate=False)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"MaU2_Bnf9duC","executionInfo":{"status":"ok","timestamp":1669768793790,"user_tz":-420,"elapsed":1643,"user":{"displayName":"3714_Muhammad Bastian Hanafi","userId":"06608780676813486786"}},"outputId":"71b774d5-ce2c-4b42-9722-eea48b67df92"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stdout","text":["+---+---------------+-------------------------+\n","|id |words          |features                 |\n","+---+---------------+-------------------------+\n","|0  |[c, c, a, b]   |(3,[0,1,2],[1.0,2.0,1.0])|\n","|1  |[a, b, b, c, a]|(3,[0,1,2],[2.0,1.0,2.0])|\n","+---+---------------+-------------------------+\n","\n"]}]}]}